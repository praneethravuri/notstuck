This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.dockerignore
app/config.py
app/core/enhanced_retriever.py
app/core/greeting_handler.py
app/core/query_classifier.py
app/core/rag_chain.py
app/core/rag.py
app/database/db.py
app/main.py
app/models/configs.py
app/models/enums.py
app/rag/main.py
app/routes/ask.py
app/routes/chats.py
app/routes/pdfs.py
app/routes/reset_db.py
app/routes/upload.py
app/utils/document_loader.py
app/utils/embedding_provider.py
app/vector_search_db/pinecone_db.py
Dockerfile
fly.toml
requirements.txt
tests/test_retrieval.py

================================================================
Files
================================================================

================
File: .dockerignore
================
# flyctl launch added from .venv\.gitignore
# Created by venv; see https://docs.python.org/3/library/venv.html
.venv\**\*
fly.toml
.env

================
File: app/config.py
================
import os
from dotenv import load_dotenv

load_dotenv()
# Paths
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
RAW_DATA_PATH = os.path.join(BASE_DIR, "data", "raw")
PROCESSED_DATA_PATH = os.path.join(BASE_DIR, "data", "processed")

# API Keys
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")

# Embedding Dimensions
PINECONE_EMBEDDING_DIMENSIONS = 1536

CHUNK_SIZE = 1000
CHUNK_OVERLAP = 150
EMBEDDING_MODEL = "text-embedding-3-small"

SIMILARITY_THRESHOLD = 0.99999  # 99.999% similar
EXACT_MATCH_THRESHOLD = 1.0     # 100% similar

MONGODB_URI = os.getenv("MONGODB_URI")

BM25_PATH = os.path.join(BASE_STORAGE_PATH, "bm25")

# Retrieval Configuration
DEFAULT_VECTOR_K = 6
DEFAULT_BM25_K = 4
DEFAULT_SUB_QUERIES = 4
DEFAULT_VIEWPOINTS = 4

================
File: app/core/enhanced_retriever.py
================
from typing import List, Optional
from langchain.docstore.document import Document
from app.models.enums import QueryType
from app.models.configs import RetrievalConfig

class EnhancedRetriever:
    def __init__(self, vector_retriever, bm25_retriever, llm, config: RetrievalConfig):
        """
        Initialize advanced document retriever.
        
        Args:
            vector_retriever: Vector-based document retriever
            bm25_retriever: BM25 document retriever
            llm: Language model for query enhancement
            config: Retrieval configuration
        """
        self.vector_retriever = vector_retriever
        self.bm25_retriever = bm25_retriever
        self.llm = llm
        self.config = config

    async def enhance_query(self, query: str) -> str:
        """Enhance query for better retrieval"""
        prompt = f"""Enhance this query for better information retrieval. 
        Return a single enhanced query, not a list. 
        Query: {query}"""
        response = await self.llm.ainvoke(prompt)
        return response.content.split('\n')[0] if '\n' in response.content else response.content

    async def generate_sub_queries(self, query: str) -> List[str]:
        """Generate sub-queries for comprehensive retrieval"""
        prompt = f"Generate {self.config.sub_queries} specific sub-questions for: {query}"
        response = await self.llm.ainvoke(prompt)
        return response.content.split('\n')

    async def identify_viewpoints(self, query: str) -> List[str]:
        """Identify diverse viewpoints for the query"""
        prompt = f"Identify {self.config.viewpoints} distinct viewpoints on: {query}"
        response = await self.llm.ainvoke(prompt)
        return response.content.split('\n')

    async def retrieve_documents(self, 
                                 query: str, 
                                 query_type: QueryType, 
                                 context: Optional[str] = None) -> List[Document]:
        """
        Retrieve documents based on query type and context.
        
        Args:
            query (str): User's input query
            query_type (QueryType): Classified query type
            context (Optional[str]): Additional context
        
        Returns:
            List[Document]: Retrieved and processed documents
        """
        enhanced_query = await self.enhance_query(query)
        
        if query_type == QueryType.FACTUAL:
            vector_docs = await self.vector_retriever.aget_relevant_documents(enhanced_query)
            bm25_docs = await self.bm25_retriever.aget_relevant_documents(enhanced_query)
            return self._merge_and_deduplicate(vector_docs, bm25_docs)
        
        elif query_type == QueryType.ANALYTICAL:
            sub_queries = await self.generate_sub_queries(query)
            all_docs = []
            for sub_query in sub_queries:
                docs = await self.vector_retriever.aget_relevant_documents(sub_query)
                all_docs.extend(docs)
            return self._ensure_diversity(all_docs)
        
        elif query_type == QueryType.OPINION:
            viewpoints = await self.identify_viewpoints(query)
            all_docs = []
            for viewpoint in viewpoints:
                combined_query = f"{query} {viewpoint}"
                docs = await self.vector_retriever.aget_relevant_documents(combined_query)
                all_docs.extend(docs)
            return self._ensure_diversity(all_docs)
        
        elif query_type == QueryType.CONTEXTUAL and context:
            contextualized_query = f"Given context: {context}, {query}"
            vector_docs = await self.vector_retriever.aget_relevant_documents(contextualized_query)
            bm25_docs = await self.bm25_retriever.aget_relevant_documents(contextualized_query)
            return self._merge_and_deduplicate(vector_docs, bm25_docs)
        
        else:
            return await self.vector_retriever.aget_relevant_documents(enhanced_query)

    def _merge_and_deduplicate(self, docs1: List[Document], docs2: List[Document]) -> List[Document]:
        """Merge and deduplicate documents"""
        seen = set()
        merged = []
        for doc in docs1 + docs2:
            if doc.page_content not in seen:
                seen.add(doc.page_content)
                merged.append(doc)
        return merged[:self.config.vector_k]

    def _ensure_diversity(self, docs: List[Document]) -> List[Document]:
        """Ensure document diversity"""
        return docs[:self.config.vector_k]

================
File: app/core/greeting_handler.py
================
class GreetingHandler:
    def __init__(self, llm):
        """
        Initialize greeting handler with a language model.
        
        Args:
            llm: Language model for generating responses
        """
        self.llm = llm
        self.greeting_prompt = """
        You are a friendly and engaging assistant. Respond naturally and appropriately to this casual interaction:
        
        User's message: {message}
        
        Remember to:
        - Be natural and conversational
        - Match the tone of the user
        - If they're sharing something personal, show appropriate empathy
        - If they ask for a joke, be appropriately humorous
        - If they ask how you're doing, be positive but honest about being an AI
        - Keep the response concise but engaging
        
        Response:"""

    async def handle_greeting(self, message: str) -> str:
        """
        Generate a contextual response to a greeting or casual message.
        
        Args:
            message (str): User's input message
        
        Returns:
            str: Generated conversational response
        """
        response = await self.llm.ainvoke(self.greeting_prompt.format(message=message))
        return response.content if hasattr(response, 'content') else str(response)

================
File: app/core/query_classifier.py
================
from app.models.enums import QueryType

class QueryClassifier:
    def __init__(self, llm):
        """
        Initialize query classifier with a language model.
        
        Args:
            llm: Language model for query classification
        """
        self.llm = llm
        self.classification_prompt = """
        Classify the following query into one of these categories: 
        - FACTUAL (specific, verifiable information)
        - ANALYTICAL (comprehensive analysis)
        - OPINION (subjective matters)
        - CONTEXTUAL (user-specific context)
        - GREETING (general greetings or pleasantries)
        
        Output only one word from the above categories.
        
        Query: {query}
        Classification:"""
    
    async def classify(self, query: str) -> QueryType:
        """
        Classify the query into a QueryType.
        
        Args:
            query (str): User's input query
        
        Returns:
            QueryType: Classified query type
        """
        response = await self.llm.ainvoke(self.classification_prompt.format(query=query))
        
        # Handle response extraction
        classification = response.content if hasattr(response, 'content') else str(response)
        classification = classification.upper().strip()
        
        # Normalize classification
        classification_mapping = {
            'FACT': 'FACTUAL',
            'FACTS': 'FACTUAL',
            'ANALYZE': 'ANALYTICAL',
            'ANALYSIS': 'ANALYTICAL',
            'OPINIONS': 'OPINION',
            'OPINIONATED': 'OPINION',
            'CONTEXT': 'CONTEXTUAL',
            'GREET': 'GREETING',
            'GREETINGS': 'GREETING',
            'HELLO': 'GREETING',
            'HI': 'GREETING'
        }
        
        # Extract first word and map to QueryType
        classification = classification.split()[0] if classification else 'CONTEXTUAL'
        classification = classification_mapping.get(classification, classification)
        
        try:
            return QueryType[classification]
        except KeyError:
            return QueryType.CONTEXTUAL

================
File: app/core/rag_chain.py
================
from typing import Dict, Any
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from app.models.enums import QueryType
from app.models.configs import RetrievalConfig

class EnhancedRAGChain:
    def __init__(self, retriever, classifier, greeting_handler, memory, llm):
        """
        Initialize the Enhanced RAG Chain.
        
        Args:
            retriever: Document retrieval system
            classifier: Query type classifier
            greeting_handler: Handles greeting interactions
            memory: Conversation memory
            llm: Language model
        """
        self.retriever = retriever
        self.classifier = classifier
        self.greeting_handler = greeting_handler
        self.memory = memory
        self.llm = llm
        self.prompt_template = self._create_prompt_template()

    def _create_prompt_template(self):
        """Create prompt template for RAG processing"""
        system_template = """You are an interactive research Analyst that helps users by finding and sharing relevant information. 
        Use the following pieces of context to answer the user's question.
        If you don't know the answer, just say that you don't know, don't try to make up an answer. 
        Cross check the answer with query/question before confirming.
        
        Query Type: {query_type}
        Context: {context}
        Question: {question}
        
        Additional Instructions based on query type:
        - For Factual queries: Provide precise, verifiable information
        - For Analytical queries: Break down the analysis step by step
        - For Opinion queries: Present different viewpoints and perspectives
        - For Contextual queries: Consider the user's specific context
        - For Greetings: Respond warmly and professionally
        
        ALWAYS include SOURCES in your answer.
        """
        return ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template("{question}")
        ])

    async def process_query(self, query: str, context: str = None) -> Dict[str, Any]:
        """
        Process user query through RAG pipeline.
        
        Args:
            query (str): User's input query
            context (str, optional): Conversation context
        
        Returns:
            Dict[str, Any]: Processing result with answer, sources, and query type
        """
        query_type = await self.classifier.classify(query)
        
        # Handle greeting separately
        if query_type == QueryType.GREETING:
            greeting_response = await self.greeting_handler.handle_greeting(query)
            return {
                "answer": greeting_response,
                "sources": [],
                "query_type": query_type
            }
        
        # Retrieve documents
        docs = await self.retriever.retrieve_documents(query, query_type, context)
        
        # Generate response
        context_text = "\n".join(doc.page_content for doc in docs)
        response = await self.llm.ainvoke(
            self.prompt_template.format(
                query_type=query_type.value,
                context=context_text,
                question=query
            )
        )

        return {
            "answer": response,
            "sources": docs,
            "query_type": query_type
        }

================
File: app/core/rag.py
================
from openai import OpenAI
from ..vector_search_db.pinecone_db import PineconeDB
from langchain_community.retrievers import BM25Retriever
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from ..config import (
    PINECONE_API_KEY,
    PINECONE_ENV,
    PINECONE_INDEX_NAME,
    PINECONE_EMBEDDING_DIMENSIONS,
    OPENAI_API_KEY
)
from ..utils.embedding_provider import OpenAIEmbeddingProvider

client = OpenAI()
client.api_key = OPENAI_API_KEY

================
File: app/database/db.py
================
# backend/app/services/db.py

from motor.motor_asyncio import AsyncIOMotorClient
from datetime import datetime
from bson.objectid import ObjectId
from app.config import MONGODB_URI

# Create a Mongo client and get the default database.
client = AsyncIOMotorClient(MONGODB_URI)
db = client.get_database("notstuck")
chats_collection = db.get_collection("chats")

async def create_chat_session() -> str:
    """Create a new chat session document with an empty messages list."""
    chat_doc = {
        "messages": [],
        "created_at": datetime.utcnow()
    }
    result = await chats_collection.insert_one(chat_doc)
    return str(result.inserted_id)

async def append_message_to_chat(chat_id: str, message: dict) -> int:
    """Append a message (with a timestamp) to the chat session identified by chat_id."""
    message["timestamp"] = datetime.utcnow()
    result = await chats_collection.update_one(
        {"_id": ObjectId(chat_id)},
        {"$push": {"messages": message}}
    )
    return result.modified_count

async def get_chat_by_id(chat_id: str) -> dict:
    """Retrieve a single chat session by its ID."""
    chat = await chats_collection.find_one({"_id": ObjectId(chat_id)})
    if chat:
        chat["chatId"] = str(chat["_id"])
        del chat["_id"]
    return chat

async def get_all_chats() -> list:
    """Return a list of all chat sessions (with minimal info)."""
    chats = []
    async for chat in chats_collection.find({}):
        chat["chatId"] = str(chat["_id"])
        del chat["_id"]
        chats.append(chat)
    return chats

================
File: app/main.py
================
# app/main.py
# uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload 
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import os

# Import the routers
from app.routes import ask, pdfs, upload, reset_db, chats


app = FastAPI()

# Set up CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# @app.on_event("startup")
# async def startup_event():
#     host = os.getenv("HOST", "0.0.0.0")
#     port = os.getenv("PORT", "8000")
#     complete_url = f"http://{host}:{port}"

# Include routers with a common prefix (e.g., /api)
app.include_router(ask.router, prefix="/api")
app.include_router(pdfs.router, prefix="/api")
app.include_router(upload.router, prefix="/api")
app.include_router(reset_db.router, prefix="/api")
app.include_router(chats.router, prefix="/api")

================
File: app/models/configs.py
================
from dataclasses import dataclass
from typing import Optional

@dataclass
class RetrievalConfig:
    """Configuration for document retrieval strategies"""
    vector_k: int = 6
    bm25_k: int = 4
    sub_queries: int = 4
    viewpoints: int = 4
    chunk_size: int = 1500
    chunk_overlap: int = 300

================
File: app/models/enums.py
================
from enum import Enum, auto
from typing import List, Dict, Any

class QueryType(Enum):
    """
    Enumeration of different query types for classification
    """
    FACTUAL = "factual"
    ANALYTICAL = "analytical"
    OPINION = "opinion"
    CONTEXTUAL = "contextual"
    GREETING = "greeting"

    @classmethod
    def get_processing_instructions(cls) -> Dict[str, str]:
        """
        Provides processing instructions for each query type
        """
        return {
            cls.FACTUAL: "Provide precise, verifiable information",
            cls.ANALYTICAL: "Break down the analysis step by step",
            cls.OPINION: "Present different viewpoints and perspectives",
            cls.CONTEXTUAL: "Consider the user's specific context",
            cls.GREETING: "Respond warmly and professionally"
        }

================
File: app/rag/main.py
================
# app/services/rag/main.py

from app.core.enhanced_retriever import EnhancedRetriever
from app.core.query_classifier import QueryClassifier
from app.core.greeting_handler import GreetingHandler
from app.core.rag_chain import EnhancedRAGChain
from app.database.db import create_chat_session, append_message_to_chat
from app.config import OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENV, PINECONE_INDEX_NAME, PINECONE_EMBEDDING_DIMENSIONS, CHUNK_SIZE, CHUNK_OVERLAP
from app.vector_search_db.pinecone_db import PineconeDB
from langchain_community.retrievers import BM25Retriever  # make sure to configure this if needed
from langchain.memory import ConversationBufferWindowMemory
from app.utils.embedding_provider import OpenAIEmbeddingProvider
from openai import OpenAI

def initialize_llm():
    # Initialize your OpenAI LLM
    llm = OpenAI(api_key=OPENAI_API_KEY)
    return llm

def get_rag_chain():
    # Instantiate the LLM
    llm = initialize_llm()

    # Set up the vector retriever using PineconeDB.
    pinecone_db = PineconeDB(api_key=PINECONE_API_KEY, environment=PINECONE_ENV, index_name=PINECONE_INDEX_NAME, embedding_dimensions=PINECONE_EMBEDDING_DIMENSIONS)
    
    # For BM25, instantiate with the necessary parameters.
    bm25_retriever = BM25Retriever(  # Ensure you configure BM25Retriever properly, e.g., with your corpus or embedding settings.
        # You might need to pass in the document store or corpus here.
    )
    
    # Configure retrieval parameters.
    from models.configs import RetrievalConfig  # assuming you have a config for retrieval parameters
    retrieval_config = RetrievalConfig(
        sub_queries=3,        # number of sub-queries to generate (adjust as needed)
        viewpoints=3,         # number of viewpoints for opinion queries
        vector_k=5            # number of documents to retrieve
    )
    
    # Create the enhanced retriever.
    enhanced_retriever = EnhancedRetriever(
        vector_retriever=pinecone_db,  # assuming your PineconeDB has the required agnostic interface
        bm25_retriever=bm25_retriever,
        llm=llm,
        config=retrieval_config
    )
    
    # Instantiate the query classifier and greeting handler.
    classifier = QueryClassifier(llm)
    greeting_handler = GreetingHandler(llm)
    
    # Set up conversation memory.
    memory = ConversationBufferWindowMemory(k=5)
    
    # Initialize the Enhanced RAG Chain.
    rag_chain = EnhancedRAGChain(
        retriever=enhanced_retriever,
        classifier=classifier,
        greeting_handler=greeting_handler,
        memory=memory,
        llm=llm
    )
    
    return rag_chain

def answer_question(
    question: str,
    top_k: int,
    threshold: float,
    temperature: float,
    max_tokens: int,
    response_style: str,
    namespace: str,
    model_name: str,
    reasoning: bool = False,
    context: str = None
):
    """
    This function is called by your /ask API route. It uses the RAG chain to process the question.
    """
    rag_chain = get_rag_chain()
    
    # Process the query through the RAG pipeline
    result = rag_chain.process_query(question, context)
    
    # You can post-process the result here (e.g., format the answer or attach source references)
    answer = result.get("answer")
    relevant_chunks = [doc.page_content for doc in result.get("sources", [])]
    
    return {
        "answer": answer,
        "relevant_chunks": relevant_chunks,
        "query_type": result.get("query_type")
    }

================
File: app/routes/ask.py
================
# backend/app/routes/ask.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
from app.rag.main import answer_question  # adjust the import path as needed
from app.database.db import create_chat_session, append_message_to_chat
from datetime import datetime

router = APIRouter()

class QuestionPayload(BaseModel):
    question: str
    similarityThreshold: float
    similarResults: int
    temperature: float
    maxTokens: int
    responseStyle: str
    modelName: str
    chatId: Optional[str] = None  # New field for chat session

@router.post("/ask")
async def ask_question(payload: QuestionPayload):
    try:
        # Determine the chat session: use provided chatId or create a new session.
        if payload.chatId:
            chat_id = payload.chatId
        else:
            chat_id = await create_chat_session()
        
        # Store the user message
        await append_message_to_chat(chat_id, {
            "role": "user",
            "content": payload.question
        })
        
        # Get the AI answer
        result = answer_question(
            question=payload.question,
            top_k=payload.similarResults,
            threshold=payload.similarityThreshold,
            temperature=payload.temperature,
            max_tokens=payload.maxTokens,
            response_style=payload.responseStyle,
            namespace="my-namespace",
            model_name=payload.modelName
        )
        
        # Store the AI answer
        await append_message_to_chat(chat_id, {
            "role": "ai",
            "content": result["answer"]
        })

        # Return the answer along with the chat session ID
        return {
            "chatId": chat_id,
            "answer": result["answer"],
            "relevant_chunks": result.get("relevant_chunks", []),
            "source_files": result.get("source_files", [])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

================
File: app/routes/chats.py
================
# backend/app/routes/chats.py

from fastapi import APIRouter, HTTPException
from app.database.db import get_all_chats, get_chat_by_id

router = APIRouter()

@router.get("/chats")
async def list_chats():
    """Return a list of all chat sessions."""
    chats = await get_all_chats()
    return chats

@router.get("/chats/{chat_id}")
async def get_chat(chat_id: str):
    """Return the conversation history for a specific chat session."""
    chat = await get_chat_by_id(chat_id)
    if not chat:
        raise HTTPException(status_code=404, detail="Chat session not found")
    return chat

================
File: app/routes/pdfs.py
================
# app/routes/pdfs.py

import os
from typing import Optional
from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import FileResponse

router = APIRouter()

# Define directory paths (adjust based on your structure)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
PROCESSED_DIR = os.path.join(DATA_DIR, "processed")

@router.get("/get-pdfs")
def pdf_endpoint(filename: Optional[str] = Query(None)):
    """
    If 'filename' is not provided => Return JSON list of PDFs.
    If 'filename' is provided => Return the PDF file as FileResponse.
    """
    try:
        if filename is None:
            if not os.path.isdir(PROCESSED_DIR):
                return {"files": []}
            files = [f for f in os.listdir(PROCESSED_DIR) if f.lower().endswith(".pdf")]
            return {"files": files}

        pdf_path = os.path.join(PROCESSED_DIR, filename)
        if not os.path.isfile(pdf_path) or not filename.lower().endswith(".pdf"):
            raise HTTPException(status_code=404, detail="PDF not found")
        return FileResponse(pdf_path, media_type="application/pdf")
    except Exception as e:
        if isinstance(e, HTTPException):
            raise e
        raise HTTPException(status_code=500, detail=str(e))

================
File: app/routes/reset_db.py
================
from app.vector_search_db.pinecone_db import PineconeDB
from pinecone import Pinecone, ServerlessSpec
from fastapi import APIRouter, HTTPException

router = APIRouter()
pinecone_db = PineconeDB()

@router.delete("/reset-db")
def reset_db():
    print("Reset DB route accessed")  # Debug print
    try:
        pinecone_db.delete_all_data(namespace="my-namespace")
        return "Database reset successful."
    except Exception as e:
        print("Error in reset_db:", e)
        raise HTTPException(status_code=500, detail=str(e))

================
File: app/routes/upload.py
================
# app/routes/upload.py

import os
from fastapi import APIRouter, HTTPException, UploadFile, File
from app.vector_search_db.pinecone_db import PineconeDB

router = APIRouter()
pinecone_db = PineconeDB()


# Define the upload directory (adjust the path as needed)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
UPLOAD_DIR = os.path.join(BASE_DIR, "data", "raw")
os.makedirs(UPLOAD_DIR, exist_ok=True)

@router.post("/upload")
async def upload_files(files: list[UploadFile] = File(...)):
    try:
        # Save each uploaded file
        for file in files:
            file_path = os.path.join(UPLOAD_DIR, file.filename)
            content = await file.read()
            with open(file_path, "wb") as f:
                f.write(content)
        
        # Process PDFs after upload
        # try:
        #     print("Processing and pushing PDFs to Pinecone...")
        #     pinecone_db.process(namespace="my-namespace")
        #     return {
        #         "message": "Files uploaded and processed successfully",
        #         "files": [file.filename for file in files]
        #     }
        # except Exception as process_error:
        #     return {
        #         "message": "Files uploaded but processing failed",
        #         "error": str(process_error),
        #         "files": [file.filename for file in files]
        #     }
            
    except Exception as upload_error:
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(upload_error)}")

================
File: app/utils/document_loader.py
================
from typing import List
from langchain.docstore.document import Document
import pymupdf4llm
from langchain.text_splitter import RecursiveCharacterTextSplitter

class DocumentLoader:
    @staticmethod
    async def load_pdf_documents(file_paths: List[str], chunk_size: int = 1500, chunk_overlap: int = 300) -> List[Document]:
        """
        Loads PDF documents and splits them into chunks.
        
        Args:
            file_paths (List[str]): List of file paths to PDF documents.
            chunk_size (int): Maximum size of each text chunk.
            chunk_overlap (int): Number of overlapping characters between chunks.
        
        Returns:
            List[Document]: A list of Document objects containing text chunks and metadata.
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        documents = []
        for file_path in file_paths:
            # Convert the PDF file to markdown text.
            pdf_text = pymupdf4llm.to_markdown(file_path)
            
            # Split the text into chunks.
            texts = text_splitter.split_text(pdf_text)
            
            # Wrap each chunk in a Document with metadata.
            docs = [
                Document(
                    page_content=text,
                    metadata={"source": file_path}
                )
                for text in texts
            ]
            documents.extend(docs)
        
        return documents

================
File: app/utils/embedding_provider.py
================
from langchain_community.embeddings.openai import OpenAIEmbeddings
from ..config import OPENAI_API_KEY, EMBEDDING_MODEL

class OpenAIEmbeddingProvider:
    def __init__(self, api_key: str = OPENAI_API_KEY, model: str = None):
        """
        Initializes the embedding provider with the OpenAI API key and model name.
        If no model is provided, defaults to "text-embedding-ada-002".
        """
        self.api_key = api_key
        self.model = model or "text-embedding-ada-002"
        self.embedding_instance = OpenAIEmbeddings(openai_api_key=self.api_key, model=self.model)

    def get_embedding_function(self) -> OpenAIEmbeddings:
        """
        Returns the OpenAI embedding callable instance.
        """
        return self.embedding_instance

================
File: app/vector_search_db/pinecone_db.py
================
import logging
import uuid
import os
import shutil
from typing import Optional, List
from pinecone import Pinecone, ServerlessSpec

from ..config import (
    PINECONE_API_KEY,
    PINECONE_ENV,
    PINECONE_INDEX_NAME,
    PINECONE_EMBEDDING_DIMENSIONS,
    SIMILARITY_THRESHOLD,
    EXACT_MATCH_THRESHOLD,
    RAW_DATA_PATH,
    PROCESSED_DATA_PATH,
)
from langchain.docstore.document import Document
from app.utils.embedding_provider import OpenAIEmbeddingProvider
from app.utils.document_loader import DocumentLoader

logger = logging.getLogger(__name__)


class PineconeDB:
    def __init__(
        self,
        api_key: Optional[str] = None,
        environment: Optional[str] = None,
        index_name: Optional[str] = None,
        embedding_dimensions: Optional[int] = None,
    ):
        """
        Initializes the PineconeDB instance by setting up the Pinecone API
        and ensuring the specified index exists.
        """
        self.api_key = api_key or PINECONE_API_KEY
        self.environment = environment or PINECONE_ENV
        self.index_name = index_name or PINECONE_INDEX_NAME
        self.embedding_dimensions = embedding_dimensions or PINECONE_EMBEDDING_DIMENSIONS
        
        # Initialize the embedding provider.
        embedding_provider = OpenAIEmbeddingProvider()
        self.embedding_function = embedding_provider.get_embedding_function()

        if not self.api_key or not self.environment:
            raise ValueError("Missing Pinecone credentials. Check config or environment variables.")

        # Initialize the Pinecone client.
        self.pc = Pinecone(api_key=self.api_key)
        self.index = self._init_index()

    def _init_index(self):
        """
        Ensures that the Pinecone index exists. If it does not, it will be created.
        """
        existing_indexes = [idx.name for idx in self.pc.list_indexes()]
        if self.index_name not in existing_indexes:
            self.pc.create_index(
                name=self.index_name,
                dimension=self.embedding_dimensions,
                metric="cosine",
                spec=ServerlessSpec(
                    region=self.environment,
                    cloud="aws"
                )
            )
            logger.info(f"Created new Pinecone index: {self.index_name}")
        else:
            logger.info(f"Using existing Pinecone index: {self.index_name}")

        return self.pc.Index(self.index_name)

    def delete_all_data(self, namespace: Optional[str] = None) -> None:
        """
        Deletes all data from the specified namespace in the Pinecone index.
        If no namespace is provided, deletes data from the default namespace.
        """
        self.index.delete(delete_all=True, namespace=namespace)
        logger.info(f"Deleted all data{' in namespace ' + namespace if namespace else ''}.")

    def upsert_documents(self, documents: List[Document], namespace: Optional[str] = None) -> None:
        """
        Upserts a list of Document objects into the Pinecone index.
        Each document is embedded and then stored as a vector in the index.
        
        Args:
            documents (List[Document]): List of Document objects.
            namespace (Optional[str]): The namespace in the Pinecone index.
        """
        texts = [doc.page_content for doc in documents]
        embeddings = self.embedding_function.embed_documents(texts)
        
        vectors = []
        for doc, embedding in zip(documents, embeddings):
            vector_id = uuid.uuid4().hex
            vectors.append({
                "id": vector_id,
                "values": embedding,
                "metadata": doc.metadata,
            })
        
        self.index.upsert(vectors=vectors, namespace=namespace)
        logger.info(f"Upserted {len(vectors)} vectors to Pinecone index '{self.index_name}' in namespace '{namespace}'.")

    def upsert_documents_with_similarity_check(self, documents: List[Document], namespace: Optional[str] = None) -> None:
        """
        Upserts a list of Document objects into the Pinecone index with a similarity check.
        For each document, its embedding is generated and the index is queried to check for similar vectors.
        If a similar vector is found with a similarity score greater than or equal to SIMILARITY_THRESHOLD,
        the document is skipped to avoid duplicate entries.
        
        Args:
            documents (List[Document]): List of Document objects.
            namespace (Optional[str]): The namespace in the Pinecone index.
        """
        vectors_to_upsert = []
        for doc in documents:
            text = doc.page_content
            # Generate embedding for the document text.
            embedding = self.embedding_function.embed_query(text)
            
            # Query the Pinecone index for the most similar vector.
            query_result = self.index.query(
                vector=embedding,
                top_k=1,
                include_values=False,
                namespace=namespace
            )
            
            skip_document = False
            if query_result and query_result.matches:
                best_match = query_result.matches[0]
                similarity_score = best_match.get("score", 0)
                # If the similarity score exceeds the threshold, skip upserting this document.
                if similarity_score >= SIMILARITY_THRESHOLD:
                    logger.info(
                        f"Skipping upsert for document from {doc.metadata.get('source', 'unknown source')} "
                        f"due to similarity score {similarity_score} >= threshold {SIMILARITY_THRESHOLD}."
                    )
                    skip_document = True
            
            if not skip_document:
                vector_id = uuid.uuid4().hex
                vectors_to_upsert.append({
                    "id": vector_id,
                    "values": embedding,
                    "metadata": doc.metadata,
                })
        
        if vectors_to_upsert:
            self.index.upsert(vectors=vectors_to_upsert, namespace=namespace)
            logger.info(
                f"Upserted {len(vectors_to_upsert)} vectors into Pinecone index '{self.index_name}' in namespace '{namespace}' after similarity check."
            )
        else:
            logger.info("No new documents to upsert after similarity check.")


def move_file_to_processed(file_name: str) -> None:
    """
    Moves a file from the raw directory to the processed directory.
    
    Args:
        file_name (str): The file name to move.
    """
    raw_file_path = os.path.join(RAW_DATA_PATH, file_name)
    processed_file_path = os.path.join(PROCESSED_DATA_PATH, file_name)
    os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)
    try:
        shutil.move(raw_file_path, processed_file_path)
        logger.info(f"Moved file '{raw_file_path}' to '{processed_file_path}'.")
    except Exception as e:
        logger.error(f"Failed to move file '{raw_file_path}' to '{processed_file_path}': {e}")


# ======================
# Example Usage
# ======================
if __name__ == "__main__":
    import asyncio
    
    async def main():
        # List of PDF file names (assumed to be located in RAW_DATA_PATH).
        pdf_file_names = ["document1.pdf", "document2.pdf"]
        # Build full paths for document loader if needed,
        # or pass file names if your loader constructs full paths from RAW_DATA_PATH.
        pdf_file_paths = [os.path.join(RAW_DATA_PATH, file_name) for file_name in pdf_file_names]
        
        # Load and split PDF documents into chunks.
        documents = await DocumentLoader.load_pdf_documents(pdf_file_paths)
        
        # Initialize the PineconeDB instance.
        pinecone_db = PineconeDB()
        
        # Option: Upsert with similarity check to avoid duplicates.
        pinecone_db.upsert_documents_with_similarity_check(documents, namespace="my-namespace")
        
        # After successful upsert, move each file from raw to processed directory.
        for file_name in pdf_file_names:
            move_file_to_processed(file_name)
    
    asyncio.run(main())

================
File: Dockerfile
================
# Use a slim Python image for a lightweight backend container
FROM python:3.10-slim

# Prevent Python from writing .pyc files to disk and buffer stdout/stderr
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# Install OS-level dependencies
RUN apt-get update && apt-get install -y build-essential && rm -rf /var/lib/apt/lists/*

# Copy the requirements file from the backend directory (which is now the context root)
COPY requirements.txt .

# Create a virtual environment and add it to PATH
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip and install Python dependencies
RUN pip install --upgrade pip && pip install -r requirements.txt

# Copy the rest of the backend code
COPY . ./

# Expose the port that FastAPI (via Uvicorn) will listen on
EXPOSE 8000

# Start the FastAPI application with Uvicorn
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

================
File: fly.toml
================
# fly.toml app configuration file generated for notstuck-backend on 2025-02-11T19:15:30-06:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'notstuck-backend'
primary_region = 'dfw'

[build]
  dockerfile = "Dockerfile"

[http_service]
  internal_port = 8000
  force_https = true
  auto_stop_machines = 'stop'
  auto_start_machines = true
  min_machines_running = 1
  processes = ['app']

[[vm]]
  memory = '1gb'
  cpu_kind = 'shared'
  cpus = 1

================
File: requirements.txt
================
docx2pdf # convert all docx to pdf
Pillow # for document conversion
python-dotenv # for environment variables
langchain
langchain-community
langchain-openai
pypdf
openai
pinecone-client
fastapi
pydantic
uvicorn
python-multipart
motor
pytest
pymupdf4llm
sentence-transformers

================
File: tests/test_retrieval.py
================
import pytest
import logging
import os
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import the answer_question function for testing
from app.services.rag.main import answer_question

# --- Create fake implementations for testing purposes ---
class FakeIndex:
    def query(self, vector, top_k, include_metadata, namespace):
        # Return a fake query response with predetermined matches.
        return {
            "matches": [
                {
                    "id": "fake-id-1",
                    "score": 0.95,
                    "metadata": {
                        "text": "This is a sample passage about production readiness.",
                        "source_file": "sample.pdf"
                    }
                },
                {
                    "id": "fake-id-2",
                    "score": 0.90,
                    "metadata": {
                        "text": "Another relevant passage about test queries.",
                        "source_file": "sample2.pdf"
                    }
                }
            ]
        }
    
    def upsert(self, vectors, namespace):
        pass

def fake_init_pinecone():
    return FakeIndex()

def fake_embed_query(query: str):
    # Return a dummy embedding vector (the exact values are not important for the fake test)
    return [0.1] * 1536

class FakeEmbedding:
    def embed_query(self, query: str):
        return fake_embed_query(query)
    
    def embed_documents(self, docs):
        # Return dummy embeddings for each document in the list.
        return [[0.1] * 1536 for _ in docs]

def fake_get_embedding_function():
    return FakeEmbedding()

# Monkey-patch the functions in the modules used by answer_question.
import app.services.rag.main as rag_main
rag_main.init_pinecone = fake_init_pinecone

import app.services.embeddings.generate_embeddings as emb_gen
emb_gen.get_embedding_function = fake_get_embedding_function

# --- Define sample test queries ---
@pytest.mark.parametrize("query", [
    "How do I ensure production readiness?",
    "What steps are needed for a robust system?",
    "How can I test retrieval performance?"
])
def test_answer_question(query):
    result = rag_main.answer_question(
        question=query,
        top_k=5,
        threshold=0.85,
        temperature=0.7,
        max_tokens=150,
        response_style="detailed",
        namespace="test-namespace",
        model_name="gpt-3.5-turbo",
        reasoning=False
    )
    # Check that an answer is returned and that relevant_chunks contains entries.
    assert "answer" in result
    assert "relevant_chunks" in result
    assert len(result["relevant_chunks"]) > 0

    # For retrieval relevance, check that a known passage is among the fetched results.
    expected_passage = "This is a sample passage about production readiness."
    found = any(expected_passage in chunk for chunk in result["relevant_chunks"])
    assert found, "Expected passage not found in the top results."

    # Optionally, log the result for debugging.
    logging.getLogger(__name__).info(f"Test query: '{query}' returned: {result}")



================================================================
End of Codebase
================================================================
